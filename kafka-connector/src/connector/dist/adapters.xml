<?xml version="1.0"?>

<!--
    This is the configuration file of the Lightstreamer Kafka Connector pluggable into Lightstreamer Server.


    A very simple variable-expansion feature is available; see
    <enable_expansion_for_adapters_config> in the Server's main configuration file.
-->

<!-- Mandatory. Define the Kafka Connector Adapter Set and its unique ID. -->
<adapters_conf id="KafkaConnector">
    <metadata_provider>
        <!-- Mandatory. Java class name of the Kafka Connector Metadata Adapter. It is possible to provide a custom implementation 
             by extending the com.lightstreamer.kafka_connector.adapters.pub.KafakConnectorMetadataAdapter class. -->
        <adapter_class>com.lightstreamer.kafka_connector.adapters.ConnectorMetadataAdapter</adapter_class>

        <!-- Mandatory. The path of the reload4j configuration file, relative to the deployment folder 
             (LS_HOME/adapters/lightstreamer-kafka-connector). -->
        <param name="logging.configuration.path">log4j.properties</param>

    </metadata_provider>

    <!-- Mandatory. The Lightstreamer Kafka Connector allows the configuration of different independent connections to different 
         Kafka clusters. 

         Every single connection is configured via the definition of its own Lightstreamer Data Adapter. At least one connection 
         configuration must be provided.
         
         Since the Connector manages the physical connection to Kafka by wrapping an internal Kafka Consumer, many
         configuration settings in the Data Adapter are identical to those required by the usual Kafka Consumer
         configuration.

         The Kafka Connector leverages the "name" attribute of the <data_provider> tag as the connection name, which will 
         be used by the Clients to request real-time data from this specific Kafka connection through a Subscription object.

         The connection name is also used to group all logging messages belonging to the same connection
         Its default value is "DEFAULT", but only one "DEFAULT" configuration is permitted. -->
    <data_provider name="BrokerConnection">
        
        <!-- Java class name of the Kafka Connector Data Adapter. DO NOT EDIT IT. -->
        <adapter_class>com.lightstreamer.kafka_connector.adapters.ConnectorDataAdapter</adapter_class>

        <!-- Optional. Enabling flag. Can be one of the following:
                - "true": Enables the connection.
                - "false": Disables the connection.
                Default: "true". -->
        <!--
        <param name="enable">false</param>
        -->        

        <!-- Mandatory. The Kafka Cluster bootstrap server endpoint expressed as the list of host/port pairs used to 
             establish the initial connection.
 
             The parameter sets the value of the "bootstrap.servers" key to configure the internal Kafka Consumer.
             See https://kafka.apache.org/documentation/#consumerconfigs_bootstrap.servers for more details.
         -->
        <param name="bootstrap.servers">broker:29092,broker:29093</param>

        <!-- Optional. The name of the consumer group this connection belongs to.
             The parameter sets the value for the "group.id" key used to configure the internal
             Kafka Consumer. See https://kafka.apache.org/documentation/#consumerconfigs_group.id for more details.
             
             Default:  Adapter Set id + the Data Adapter name + randomly generated suffix. -->
        <!--
        param name="group.id">kafka-connector-group</param>
        -->

        <!-- Optional. The error handling strategy to be used if an error occurs while extracting data from incoming records.
                Can be one of the following:
                - "IGNORE_AND_CONTINUE": Ignore the error and continue to process the next record.
                - "FORCE_UNSUBSCRIPTION": Stop processing records and force unsubscription of the items 
                  requested by all Lightstreamer clients subscribed to this connection.
                Default: "IGNORE_AND_CONTINUE". -->
        <!-- 
        <param name="record.extraction.error.strategy">FORCE_UNSUBSCRIPTION</param>
        -->        

        <!-- TOPIC MAPPING SECTION -->

        <!-- Define a "sample" item-template, which is simply made of the "sample" item name to be used by the Lighstreamer Client subscription. -->
        <param name="item-template.sample1">sample-#{partition=PARTITION}</param>
        <param name="item-template.sample2">sample</param>

        <!-- Map the Kafka topic "sample-topic" to the previous defined "sample" item template. -->
        <param name="map.sample-topic.to">item-template.sample1,item-template.sample2</param>
        <!--<param name="map.sample-topic2.to">item-template.sample2</param>-->

        <!-- <param name="encryption.enable">true</param> -->
        <!--<param name="encryption.security.protocol">SSL</param>-->
        <!-- <param name="encryption.truststore.path">secrets/kafka.client.truststore.jks1</param>
        <param name="encryption.truststore.password">password</param> -->
        <param name="encryption.truststore.path">secrets/kafka.consumer.truststore.jks</param>
        <param name="encryption.truststore.password">confluent</param>
        <!--<param name="encryption.keystore.enable">true</param>-->
        <param name="encryption.keystore.path">secrets/kafka.consumer.keystore.jks</param>
        <param name="encryption.keystore.password">confluent</param>

        <!-- FIELDS MAPPING SECTION -->

        <!-- Extraction of the record key mapped to the field "key". -->
        <param name="field.key">#{KEY}</param>

        <!-- Extraction of the record value mapped to the field "value". -->
        <param name="field.value">#{VALUE}</param>

        <!-- Extraction of the record timestamp to the field "ts". -->
        <param name="field.ts">#{TIMESTAMP}</param>

        <!-- Extraction of the record partition mapped to the field "partition". -->
        <param name="field.partition">#{PARTITION}</param>

        <!-- Extraction of the record offset mapped to the field "offset". -->
        <param name="field.offset">#{OFFSET}</param>

        <!---->
    </data_provider>

    <data_provider name="QuickStart">
        <adapter_class>com.lightstreamer.kafka_connector.adapters.ConnectorDataAdapter</adapter_class>

        <!-- The Kafka cluster address -->
        <!--<param name="bootstrap.servers">pkc-z9doz.eu-west-1.aws.confluent.cloud:9092</param>-->
        <param name="bootstrap.servers">broker:29092</param>

        <param name="group.id">test</param>

        <param name="enable">true</param>

        <!-- TOPIC MAPPING SECTION -->

        <!-- Define a "sample" item-template, which is simply made of the "sample" item name to be used by the Lighstreamer Client subscription. -->
        <!-- <param name="item-template.sample1">sample-#{partition=PARTITION}</param>
        <param name="item-template.sample2">sample</param> -->
        <param name="item-template.sample1">sample-#{key=KEY.name}</param>
        <param name="item-template.sample2">sample-#{value=VALUE}</param>

        <!-- Map the Kafka topic "sample-topic" to the previous defined "sample" item template. -->
        <param name="map.avro-topic-1.to">item-template.sample1</param>

        <param name="key.evaluator.type">AVRO</param>
        <!-- <param name="key.evaluator.schema.registry.enable">true</param> -->
        <param name="key.evaluator.schema.path">user-key.avsc</param>
        <param name="value.evaluator.type">AVRO</param>
        <!-- <param name="value.evaluator.schema.registry.enable">true</param> -->
        <param name="value.evaluator.schema.path">user-value.avsc</param>

        <!-- FIELDS MAPPING SECTION -->

        <!-- Extraction of the record key mapped to the field "key". -->
        <param name="field.key">#{KEY.name}</param>

        <!-- Extraction of the record value mapped to the field "value". -->
        <param name="field.value">#{VALUE.name}</param>

        <!-- Extraction of the record timestamp to the field "ts". -->
        <param name="field.ts">#{TIMESTAMP}</param>

        <!-- Extraction of the record partition mapped to the field "partition". -->
        <param name="field.partition">#{PARTITION}</param>

        <!-- Extraction of the record offset mapped to the field "offset". -->
        <param name="field.offset">#{OFFSET}</param>

    </data_provider> 

</adapters_conf>

