<?xml version="1.0"?>

<!--
    This is the configuration file of the Lightstreamer Kafka Connector pluggable into Lightstreamer Server.


    A very simple variable-expansion feature is available; see
    <enable_expansion_for_adapters_config> in the Server's main configuration file.
-->

<!-- Mandatory. Define the Kafka Connector Adapter Set and its unique ID. -->
<adapters_conf id="KafkaConnector">
    <metadata_provider>
        <!-- Mandatory. Java class name of the Kafka Connector Metadata Adapter. It is possible to provide a
             custom implementation by extending the this class. -->
        <adapter_class>com.lightstreamer.kafka_connector.adapters.pub.KafkaConnectorMetadataAdapter</adapter_class>

        <!-- Mandatory. The path of the reload4j configuration file, relative to the deployment folder
             (LS_HOME/adapters/lightstreamer-kafka-connector). -->
        <param name="logging.configuration.path">log4j.properties</param>

    </metadata_provider>

    <!-- <data_provider name="BrokerConnection"> -->
        <!-- Define a "sample" item-template, which is simply made of the "sample" item name to be used by the Lighstreamer Client subscription. -->
        <!-- <param name="item-template.sample1">sample-#{partition=PARTITION}</param> -->
        <!-- <param name="item-template.sample2">sample</param> -->

        <!-- Map the Kafka topic "sample-topic" to the previous defined "sample" item template. -->
        <!-- <param name="map.sample-topic.to">item-template.sample1,item-template.sample2</param> -->
        <!--<param name="map.sample-topic2.to">item-template.sample2</param>-->

        <!-- <param name="encryption.enable">true</param> -->
        <!--<param name="encryption.security.protocol">SSL</param>-->
        <!-- <param name="encryption.truststore.path">secrets/kafka.client.truststore.jks1</param>
        <param name="encryption.truststore.password">password</param> -->
        <!-- <param name="encryption.truststore.path">secrets/kafka.consumer.truststore.jks</param>
        <param name="encryption.truststore.password">confluent</param> -->
        <!--<param name="encryption.keystore.enable">true</param>-->
        <!-- <param name="encryption.keystore.path">secrets/kafka.consumer.keystore.jks</param>
        <param name="encryption.keystore.password">confluent</param> -->

        <!-- FIELDS MAPPING SECTION -->

        <!-- Extraction of the record key mapped to the field "key". -->
        <!-- <param name="field.key">#{KEY}</param> -->
        <!-- Extraction of the record value mapped to the field "value". -->
        <!-- <param name="field.value">#{VALUE}</param> -->
        <!-- Extraction of the record timestamp to the field "ts". -->
        <!-- <param name="field.ts">#{TIMESTAMP}</param> -->
        <!-- Extraction of the record partition mapped to the field "partition". -->
        <!-- <param name="field.partition">#{PARTITION}</param> -->
        <!-- Extraction of the record offset mapped to the field "offset". -->
        <!-- <param name="field.offset">#{OFFSET}</param> -->
    <!-- </data_provider> -->

    <!-- Mandatory. The Lightstreamer Kafka Connector allows the configuration of different independent connections to different
         Kafka clusters.

         Every single connection is configured via the definition of its own Lightstreamer Data Adapter. At least one connection
         configuration must be provided.

         Since the Connector manages the physical connection to Kafka by wrapping an internal Kafka Consumer, many
         configuration settings in the Data Adapter are identical to those required by the usual Kafka Consumer
         configuration.

         The Kafka Connector leverages the "name" attribute of the <data_provider> tag as the connection name, which will
         be used by the Clients to request real-time data from this specific Kafka connection through a Subscription object.

         The connection name is also used to group all logging messages belonging to the same connection
         Its default value is "DEFAULT", but only one "DEFAULT" configuration is permitted. -->
    <data_provider name="QuickStart">

        <!-- Java class name of the Kafka Connector Data Adapter. DO NOT EDIT IT. -->
        <adapter_class>com.lightstreamer.kafka_connector.adapters.ConnectorDataAdapter</adapter_class>
        

        <!-- Optional. Enable this connection configuration. Can be one of the following:
             - true
             - false
             Default: true. -->
        <!--
        <param name="enable">false</param>
        -->

        <!-- Mandatory. The Kafka Cluster bootstrap server endpoint expressed as the list of host/port pairs used to
             establish the initial connection.

             The parameter sets the value of the "bootstrap.servers" key to configure the internal Kafka Consumer.
             See https://kafka.apache.org/documentation/#consumerconfigs_bootstrap.servers for more details.
         -->
        <param name="bootstrap.servers">broker:29092</param>

        <!-- Optional. The name of the consumer group this connection belongs to.
             The parameter sets the value for the "group.id" key used to configure the internal
             Kafka Consumer. See https://kafka.apache.org/documentation/#consumerconfigs_group.id for more details.

             Default: Adapter Set id + the Data Adapter name + randomly generated suffix. -->
        <!--
        param name="group.id">kafka-connector-group</param>
        -->
        <param name="group.id">quick-start-group</param>

        <!-- Optional. The error handling strategy to be used if an error occurs while extracting data from incoming records.
             Can be one of the following:
             - IGNORE_AND_CONTINUE: Ignore the error and continue to process the next record.
             - FORCE_UNSUBSCRIPTION: Stop processing records and force unsubscription of the items
                                     requested by all Lightstreamer clients subscribed to this connection.
             Default: "IGNORE_AND_CONTINUE". -->
        <!--
        <param name="record.extraction.error.strategy">FORCE_UNSUBSCRIPTION</param>
        -->

        <!-- Optional. The format to be used to deserialize respectively the key and value of a Kafka record.
             Can be one of the following:
             - AVRO
             - JSON
             - STRING
             - INTEGER
             - BOOLEAN
             - BYTE_ARRAY
             - BYTE_BUFFER
             - BYTES
             - DOUBLE
             - FLOAT
             - LONG
             - SHORT
             - UUID
             Default: STRING -->
        <!--
        <param name="key.evaluator.type">JSON</param>
        <param name="value.evaluator.type">JSON</param>
        -->

        <!-- TOPIC MAPPING SECTION -->

        <!-- Define the "stock" item-template, which specifies the format of the item name to be used 
             by the Lighstreamer Client subscription.
             
             All item name in the following formats:
             stock
             stock-[index=1]
             stock-[index=2]
             ...
             stock-[index=N]

         -->
        <param name="item-template.stock">stock-#{index=KEY}</param>

        <!-- Map the Kafka topic "stocks" to the previous defined "stock" item template. -->
        <param name="map.stocks.to">item-template.stock</param>

        <param name="item-template.stock">stock-#{index=KEY}</param>

        <param name="key.evaluator.type">INTEGER</param>
        <param name="value.evaluator.type">JSON</param>
        <param name="consumer.auto.offset.reset">earliest</param>

        <!-- FIELDS MAPPING SECTION -->
        <param name="field.index">#{KEY}</param>
        <param name="field.offset">#{OFFSET}</param>
        <param name="field.partition">#{PARTITION}</param>

        <!-- Extraction of the record value mapped to the field "value". -->
        <!-- Extraction of the record timestamp to the field "ts". -->
        <param name="field.time">#{VALUE.time}</param>

        <!-- Extraction of the record partition mapped to the field "partition". -->
        <param name="field.timestamp">#{VALUE.timestamp}</param>

        <!-- Extraction of the record offset mapped to the field "offset". -->
        <param name="field.stock_name">#{VALUE.name}</param>
        <param name="field.last_price">#{VALUE.last_price}</param>
        <param name="field.ask">#{VALUE.ask}</param>
        <param name="field.ask_quantity">#{VALUE.ask_quantity}</param>
        <param name="field.bid">#{VALUE.bid}</param>
        <param name="field.bid_quantity">#{VALUE.bid_quantity}</param>
        <param name="field.pct_change">#{VALUE.pct_change}</param>
        <param name="field.min">#{VALUE.min}</param>
        <param name="field.max">#{VALUE.max}</param>
        <param name="field.ref_price">#{VALUE.ref_price}</param>
        <param name="field.open_price">#{VALUE.open_price}</param>
        <param name="field.item_status">#{VALUE.item_status}</param>

        <!-- Enable encryption -->
        <!-- Optional. Enable encryption of this connection. Can be one of the following:
             - true
             - false
            Default value: false. -->
        <!--
        <param name="encryption.enable">true</param>
        -->

        <!-- Optional. The SSL protocol to be used. Can be one of the following:
             - TLSv1.2
             - TLSv1.3
             Default value: TLSv1.3 when running on Java 11 or newer, TLSv1.2 otherwise. -->
        <!--
        <param name="encryption.protocol">TLSv1.2</param>
        -->

        <!--
        <param name="encryption.enabled.protocols">TLSv1.2</param>
        <param name="encryption.cipher.suites">TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA,TLS_RSA_WITH_AES_256_CBC_SHA</param>
        <param name="encryption.hostname.verification.enable">true</param>
        -->

        <!-- If required, configure the trust store to trust the Kafka Cluster certificates -->
        <!--
        <param name="encryption.truststore.type">JKS</param>
        <param name="encryption.truststore.path">secrets/kafka.connector.truststore.pkcs12</param></param>
        <param name="encryption.truststore.path">secrets/kafka.connector.schema.registry.truststore.pkcs12</param></param>
        -->

        <!-- If mutual TLS is enabled on the Kafka Cluster, enable and configure the key store -->
        <!--
        <param name="encryption.keystore.enable">true</param>
        <param name="encryption.keystore.type">PKCS12</param>
        <param name="encryption.keystore.path">secrets/kafka.connector.encryption.keystore.pkcs12</param>
        <param name="encryption.keystore.password">schemaregistry-keystore-password</param>
        <param name="encryption.keystore.key.password">schemaregistry-private-key-password</param>
        -->

    </data_provider>

</adapters_conf>

