<?xml version="1.0"?>

<!--
    This is the configuration file of the Lightstreamer Kafka Connector pluggable into Lightstreamer Server.

    A very simple variable-expansion feature is available; see
    <enable_expansion_for_adapters_config> in the Server's main configuration file.
-->

<!-- Mandatory. Define the Kafka Connector Adapter Set and its unique ID. -->
<adapters_conf id="KafkaConnector">
    <metadata_provider>
        <!-- Mandatory. Java class name of the Kafka Connector Metadata Adapter. It is possible to provide a
             custom implementation by extending this class. -->
        <adapter_class>com.lightstreamer.kafka.adapters.pub.KafkaConnectorMetadataAdapter</adapter_class>

        <!-- Mandatory. The path of the reload4j configuration file, relative to the deployment folder
             (LS_HOME/adapters/lightstreamer-kafka-connector). -->
        <param name="logging.configuration.path">log4j.properties</param>

    </metadata_provider>

    <!-- Mandatory. The Kafka Connector allows the configuration of different independent connections to different Kafka
         broker/clusters.

         Every single connection is configured via the definition of its own Lightstreamer Data Adapter. At least one connection
         configuration must be provided.

         Since the Kafka Connector manages the physical connection to Kafka by wrapping an internal Kafka Consumer, several
         configuration settings in the Data Adapter are identical to those required by the usual Kafka Consumer
         configuration.

         The Kafka Connector leverages the "name" attribute of the <data_provider> tag as the connection name, which will
         be used by the Clients to request real-time data from this specific Kafka connection through a Subscription object.

         The connection name is also used to group all logging messages belonging to the same connection.

         Its default value is "DEFAULT", but only one "DEFAULT" configuration is permitted. -->
    <data_provider name="QuickStart">
        <!-- ##### GENERAL PARAMETERS ##### -->

        <!-- Java class name of the Kafka Connector Data Adapter. DO NOT EDIT IT. -->
        <adapter_class>com.lightstreamer.kafka.adapters.KafkaConnectorDataAdapter</adapter_class>

        <param name="bootstrap.servers">broker:29092</param>

        <param name="group.id">quick-start-group</param>

        <param name="record.consume.from">EARLIEST</param>

        <!-- Optional. Specifies when to consume records from the Kafka topics. Can be one of the following:
             - true
             - false

             If set to `true`, the Kafka Connector will start consuming records immediately after
             initialization, before any Lightstreamer client subscription request.
             If set to `false`, the Kafka Connector will conserve resources by waiting until at least
             one Lightstreamer client requests a subscription to an item mapped to a Kafka topic.

             Default value: false. -->
        <param name="record.consume.at.connector.startup.enable">true</param>

        <!-- Optional. Enables the subscription of implicit items.
             For every record whose values match any the configured item templates, an implicit item 
             will be internally created and subscribed to the Kafka Connector.

             Default value: false. -->
        <param name="record.consume.at.connector.startup.with.implicit.items.enable">true</param>

        <!-- Define the item template to enable filtering by "account_id" in the record headers -->
        <param name="item-template.positions">positions-#{account_id=HEADERS['account_id']}</param>        

        <!-- Map topic "positions" to the "positions" item template -->
        <param name="map.positions.to">item-template.positions</param>
        
        <param name="record.key.evaluator.type">STRING</param>
        <!-- Based on your example, I used JSON here, but you can change it to another format 
             like Protobuf or Avro if needed.
             If you use Protobuf or Avro, ensure the Schema Registry settings are configured accordingly.
             See https://github.com/Lightstreamer/Lightstreamer-kafka-connector?tab=readme-ov-file#record-evaluation for more information.
         -->
        <param name="record.value.evaluator.type">JSON</param>  

        <!-- Enable Auto Command Mode -->
        <param name="fields.auto.command.mode.enable">true</param>

        <!-- Map the "key" field to the record key (which is the position id).
             This is the mandatory key used by the COMMAND mode subscription
         -->
        <param name="field.key">#{KEY}</param>

        <!-- Map other fields from the record value to Lightstreamer fields -->
        <param name="field.account_id">#{HEADERS['account_id']}</param>
        <param name="field.position_id">#{KEY}</param>
        <param name="field.instrument">#{VALUE.instrument}</param>
        <param name="field.open">#{VALUE.open}</param>
        <param name="field.direction">#{VALUE.direction}</param>
        <!-- Add other mappings here -->

    </data_provider>

</adapters_conf>
